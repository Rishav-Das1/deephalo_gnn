{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import DynamicEdgeConv, global_max_pool, knn_graph\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "# Set seed\n",
    "# SEED=42\n",
    "# random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0')\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36757"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PointNetInstanceSeg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNetInstanceSeg, self).__init__()\n",
    "        self.edge_conv1 = DynamicEdgeConv(nn.Sequential(\n",
    "            nn.Linear(14, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SiLU()\n",
    "        ), k=K)\n",
    "        # self.edge_conv2 = DynamicEdgeConv(nn.Sequential(\n",
    "        #      nn.Linear(256, 512),\n",
    "        #      nn.SiLU(),\n",
    "        #      nn.Linear(512, 512),\n",
    "        #      nn.SiLU()\n",
    "        #  ), k=8)\n",
    "        # self.edge_conv3 = DynamicEdgeConv(nn.Sequential(\n",
    "        #      nn.Linear(1024, 512),\n",
    "        #      nn.SiLU(),\n",
    "        #      nn.Linear(512, 256),\n",
    "        #      nn.SiLU()\n",
    "        #  ), k=8)\n",
    "        # self.edge_conv4 = DynamicEdgeConv(nn.Sequential(\n",
    "        #      nn.Linear(256, 256),\n",
    "        #      nn.SiLU(),\n",
    "        #      nn.Linear(256, 128),\n",
    "        #      nn.SiLU()\n",
    "        #  ), k=K)\n",
    "        self.fc = nn.Linear(128, 213)  # Predicting instance mask for each point\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.pos, data.edge_index\n",
    "        # print(data.edge_index)\n",
    "        # return\n",
    "        x = self.edge_conv1(x, edge_index)\n",
    "        # x = self.edge_conv2(x, edge_index)\n",
    "        # x = self.edge_conv3(x, edge_index)\n",
    "        # x = self.edge_conv4(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def adjust_fc(self, num_classes):\n",
    "        self.fc = nn.Linear(128, num_classes = 1)\n",
    "\n",
    "# Example usage\n",
    "# model = PointNetInstanceSeg().to(DEVICE)\n",
    "\n",
    "# Assuming you have point cloud data in a DataLoader\n",
    "# Example data loading code:\n",
    "# loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# for data in loader:\n",
    "#     instance_masks = model(data)\n",
    "#     # Use instance_masks for further processing or interpretation\n",
    "\n",
    "\n",
    "# Calc number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(PointNetInstanceSeg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have point cloud data stored as a list of positions\n",
    "\n",
    "data = \"./Labeled subhalo matrices of haloes/train\"\n",
    "files = os.listdir(data)\n",
    "point_cloud_data = [(np.load(data+\"/\"+f)) for f in files if f.endswith(\".npy\")] # List of point cloud data, each element is a list of point coordinates\n",
    "\n",
    "# Convert each point cloud data into a Data object\n",
    "data_list = []\n",
    "for point_cloud in point_cloud_data:\n",
    "    # Create a Data object with the positions of points\n",
    "    pos = torch.tensor(point_cloud[:,:-1], dtype=torch.float)\n",
    "    # Recentering positions per halo\n",
    "    pos[:3] = pos[:3] - pos[:3].mean(dim=1, keepdim=True)\n",
    "    data = Data(\n",
    "        pos=pos,\n",
    "        y = torch.tensor(point_cloud[:,-1]+1, dtype=torch.long),\n",
    "        # edge_index=knn_graph(pos, k=K)\n",
    "    )\n",
    "    # data.y = torch.tensor(point_cloud[:,-1], dtype=torch.long)\n",
    "    # Dynamically generate edge_index based on the positions of points\n",
    "    # You can use a method like k-NN to construct the edges\n",
    "    # For example, using knn_graph from torch_geometric.transforms:\n",
    "    # from torch_geometric.transforms import knn_graph\n",
    "    # data.edge_index = knn_graph(data.pos, k=K)  # Construct edges based on 6 nearest neighbors\n",
    "    # Add other necessary attributes to the Data object if needed\n",
    "    # For example, data.y for ground truth labels\n",
    "    data_list.append(data)\n",
    "\n",
    "# Now you can use DataLoader with this list of Data objects\n",
    "loader = DataLoader(data_list, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((213,), 1.004488232074439)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 203\n",
    "\n",
    "weight_vec = np.repeat(1/p, p)\n",
    "weight_vec = np.concatenate([np.array([(1/p)*(1/90)]), np.repeat([(1/p)*(1/10)], 9), weight_vec])\n",
    "weight_vec.shape, weight_vec.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have point cloud data stored as a list of positions\n",
    "\n",
    "data = \"./Labeled subhalo matrices of haloes/train\"\n",
    "files = os.listdir(data)\n",
    "point_cloud_data = [(np.load(data+\"/\"+f)) for f in files if f.endswith(\".npy\")] # List of point cloud data, each element is a list of point coordinates\n",
    "\n",
    "# Convert each point cloud data into a Data object\n",
    "data_list = []\n",
    "for point_cloud in point_cloud_data:\n",
    "    # Create a Data object with the positions of points\n",
    "    pos = torch.tensor(point_cloud[:,:-1], dtype=torch.float)\n",
    "    # Recentering positions per halo\n",
    "    pos[:3] = pos[:3] - pos[:3].mean(dim=1, keepdim=True)\n",
    "    data = Data(\n",
    "        pos=pos,\n",
    "        y = torch.tensor(point_cloud[:,-1]+1, dtype=torch.long),\n",
    "        # edge_index=knn_graph(pos, k=K)\n",
    "    )\n",
    "    # data.y = torch.tensor(point_cloud[:,-1], dtype=torch.long)\n",
    "    # Dynamically generate edge_index based on the positions of points\n",
    "    # You can use a method like k-NN to construct the edges\n",
    "    # For example, using knn_graph from torch_geometric.transforms:\n",
    "    # from torch_geometric.transforms import knn_graph\n",
    "    # data.edge_index = knn_graph(data.pos, k=K)  # Construct edges based on 6 nearest neighbors\n",
    "    # Add other necessary attributes to the Data object if needed\n",
    "    # For example, data.y for ground truth labels\n",
    "    data_list.append(data)\n",
    "\n",
    "# Now you can use DataLoader with this list of Data objects\n",
    "loader = DataLoader(data_list, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 140/140 [00:29<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2137.5242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 140/140 [00:29<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 822.4704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 140/140 [00:29<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 464.1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 140/140 [00:29<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 145.3786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 140/140 [00:30<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 14.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 140/140 [00:30<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 8.9247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 140/140 [00:29<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 8.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 140/140 [00:29<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 7.2972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 140/140 [00:28<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 6.1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 140/140 [00:29<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 5.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your dataset and DataLoader\n",
    "# dataloader = DataLoader(SHDataSet(\"train\"), batch_size=1, shuffle=True)\n",
    "# Initialize the model\n",
    "model = PointNetInstanceSeg().to(DEVICE)\n",
    "weights = torch.FloatTensor(weight_vec).to(DEVICE)\n",
    "# Define your loss function and optimizer\n",
    "# Assuming instance masks are represented as class labels\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        try:\n",
    "            # print(data.pos.shape, data.pos.size(0), data.pos.numel())\n",
    "            # assert 2 == 3\n",
    "            # x.size(0) == batch_x.numel()\n",
    "            data.to(DEVICE)\n",
    "            # print(data.y.shape)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            # Assuming instance masks are represented as class labels and provided in data.y\n",
    "            # target = torch.argmax(data.y, dim=1)  # Convert one-hot encoded target to class labels\n",
    "            # loss = criterion(outputs, target)\n",
    "            loss = criterion(outputs, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.num_graphs\n",
    "        except Exception as e:\n",
    "            print(data.y.shape)\n",
    "            raise e\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Create the \"ckpts\" directory if it doesn't exist\n",
    "import os, time\n",
    "\n",
    "os.makedirs(\"ckpts\", exist_ok=True)\n",
    "curr_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{curr_time}_pointnet_instance_seg\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f\"./ckpts/{model_name}_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 30/30 [00:03<00:00,  8.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataLoader `test_loader` for your test data\n",
    "test_loader = DataLoader(data_test_list, batch_size=1, shuffle=False)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "\n",
    "# if not model_name:\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions\n",
    "ground_truth_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Loop over the test data\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc='Testing'):\n",
    "        # Move data to the device\n",
    "        data = data.to(DEVICE)\n",
    "        \n",
    "        # Pass the data through the model\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Get the predicted labels\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        \n",
    "        # Store the predictions\n",
    "        ground_truth_labels.append(data.y.cpu().numpy())\n",
    "        predictions.append(predicted_labels.cpu().numpy())\n",
    "\n",
    "# At this point, `predictions` is a list of numpy arrays with the predicted labels for each point cloud in the test set\n",
    "# You can now compare these predictions to the actual labels to compute your test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t (10065,) \t [  0   3   5   6  13  22  76 130] \t [0 1 2 3]\n",
      "\n",
      "1 \t (78716,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31]\n",
      "\n",
      "2 \t (5704,) \t [0] \t [0 1 2]\n",
      "\n",
      "3 \t (134538,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77]\n",
      "\n",
      "4 \t (14113,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "5 \t (7083,) \t [0] \t [0 1 2 3 4 5]\n",
      "\n",
      "6 \t (10423,) \t [0] \t [0 1 2]\n",
      "\n",
      "7 \t (6830,) \t [0] \t [0 1 2 3 4 5 6]\n",
      "\n",
      "8 \t (6355,) \t [0] \t [0]\n",
      "\n",
      "9 \t (13648,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "\n",
      "10 \t (12394,) \t [0] \t [0 1 2 3 4 5 6]\n",
      "\n",
      "11 \t (7559,) \t [0] \t [0 1 2 3 4 5]\n",
      "\n",
      "12 \t (16444,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "13 \t (6586,) \t [  0   2   3   5   6  10  13  14  16  20  28  32  33  39  72  76  94 130] \t [0 1 2 3]\n",
      "\n",
      "14 \t (9898,) \t [0] \t [0 1 2 3]\n",
      "\n",
      "15 \t (8716,) \t [0] \t [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "16 \t (6429,) \t [0 2 4] \t [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "17 \t (15265,) \t [0 2 3 5 6] \t [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "18 \t (18205,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "\n",
      "19 \t (13278,) \t [  0   2 130] \t [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "20 \t (67939,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
      "\n",
      "21 \t (15318,) \t [0] \t [0 1 2 3 4 5 6 7]\n",
      "\n",
      "22 \t (6788,) \t [ 0  2 15] \t [0 1 2 3 4 5]\n",
      "\n",
      "23 \t (90050,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41]\n",
      "\n",
      "24 \t (9654,) \t [0] \t [0 1 2]\n",
      "\n",
      "25 \t (5874,) \t [0 2] \t [0 1]\n",
      "\n",
      "26 \t (5374,) \t [  0   3   5   6  13  76 130] \t [0 1 2 3 4 5]\n",
      "\n",
      "27 \t (23708,) \t [0] \t [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "28 \t (10569,) \t [0 2 4] \t [0 1 2 3 4 5 6]\n",
      "\n",
      "29 \t (5412,) \t [0 4] \t [0 1 2 3 4]\n",
      "\n",
      "Mean acc: 0.8566 \\pm 0.1391\n",
      "Mean F!: 0.8318 \\pm 0.1240\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Assuming you have the ground truth labels stored in `ground_truth_labels`\n",
    "# Calculate accuracy\n",
    "accs = []\n",
    "f1s = []\n",
    "for idx, (gt, pred) in enumerate(zip(ground_truth_labels, predictions)):\n",
    "    # Checking if the model predicts different labels for different points in the same point cloud\n",
    "    # if np.unique(pred).shape[0] != 1:\n",
    "    print(idx, \"\\t\", gt.shape, \"\\t\", np.unique(pred), \"\\t\", np.unique(gt))\n",
    "    print()\n",
    "    \n",
    "    accs.append(accuracy_score(gt, pred))\n",
    "    f1s.append(f1_score(gt, pred, average='weighted'))\n",
    "\n",
    "print(f\"Mean acc: {np.mean(accs):.4f} \\pm {np.std(accs):.4f}\")\n",
    "print(f\"Mean F!: {np.mean(f1s):.4f} \\pm {np.std(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(ground_truth_labels[0], return_counts=True)\n",
    "np.unique(ground_truth_labels[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(predictions[0], return_counts=True)\n",
    "np.unique(predictions[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2864247681.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    this is meant to error out if left uncommented\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "this is meant to error out if left uncommented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, set):\n",
    "        if set == \"train\":\n",
    "            data = \"/home/group10/ml/Labeled subhalo matrices of haloes/train\"\n",
    "        elif set == \"val\":\n",
    "            data = \"/home/group10/ml/Labeled subhalo matrices of haloes/train/val\"\n",
    "        \n",
    "        self.length = len(data)\n",
    "\n",
    "        self.set = set\n",
    "        \n",
    "        files = os.listdir(data)\n",
    "        files = [torch.tensor(np.load(data+\"/\"+f), dtype=torch.float64) for f in files if f.endswith(\".npy\")]\n",
    "        files1=[f[:,:-1] for f in files]\n",
    "        self.files = files1\n",
    "        \n",
    "        labels = [f[:,-1] for f in files]\n",
    "        \n",
    "        labels = [torch.nn.functional.one_hot(j, 213) for j in labels]\n",
    "        \n",
    "        self.labels = labels\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return torch.tensor(self.files[index], dtype = torch.float64), torch.tensor(self.labels[index], dtype = torch.long)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "def train_accuracy(\n",
    "    model,\n",
    "    data_generator,\n",
    "    GPU = torch.device(\"cuda:2\"),\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accs = []\n",
    "        for batch_x, batch_y in data_generator:\n",
    "            batch_x, batch_y = batch_x.to(GPU), batch_y\n",
    "            y_true = batch_y.argmax(1).numpy()\n",
    "            y_pred = model(batch_x).argmax(1).cpu().numpy()\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            accs.append(acc*100)\n",
    "    model.train()\n",
    "    return np.array(accs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    data = np.load(os.path.join(self.directory, self.files[idx]))\n",
    "    points = torch.tensor(data[:, :3], dtype=torch.float32)  # Assuming the first 3 columns are the point coordinates\n",
    "    labels = torch.tensor(data[:, 3:], dtype=torch.long)  # Assuming the rest of the columns are the labels\n",
    "    return points, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_point_cloud(points, labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=labels, cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of data\n",
    "data = next(iter(loader))\n",
    "\n",
    "# Run the model and get the outputs\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(data)\n",
    "\n",
    "# Get the predicted labels\n",
    "_, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "# Visualize the point cloud with the predicted labels\n",
    "visualize_point_cloud(data.pos.cpu().numpy(), predicted_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in loader:\n",
    "    print(data)\n",
    "    print(data.y.shape)\n",
    "    print(data.pos.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function expects dynamic number of classes\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "max_class_index = 0  # Initialize maximum class index\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Determine the maximum class index encountered\n",
    "        max_class_index = max(max_class_index, torch.max(data.y).item())\n",
    "        \n",
    "        # Adjust the fully connected layer dynamically based on the maximum class index\n",
    "        model.adjust_fc(max_class_index + 1)  # Add 1 to account for zero-based indexing\n",
    "        \n",
    "        loss = criterion(outputs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * data.num_graphs\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# After training, you can use the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outputs.shape, data.y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
