{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import DynamicEdgeConv, global_max_pool, knn_graph\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "# Set seed\n",
    "# SEED=42\n",
    "# random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11989"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PointNetInstanceSeg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNetInstanceSeg, self).__init__()\n",
    "        self.edge_conv1 = DynamicEdgeConv(nn.Sequential(\n",
    "            nn.Linear(14, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SiLU()\n",
    "        ), k=K)\n",
    "        # self.edge_conv2 = DynamicEdgeConv(nn.Sequential(\n",
    "        #      nn.Linear(256, 128),\n",
    "        #      nn.SiLU(),\n",
    "        #      nn.Linear(128, 64),\n",
    "        #      nn.SiLU()\n",
    "        #  ), k=K)\n",
    "        # self.edge_conv3 = DynamicEdgeConv(nn.Sequential(\n",
    "        #      nn.Linear(1024, 512),\n",
    "        #      nn.SiLU(),\n",
    "        #      nn.Linear(512, 256),\n",
    "        #      nn.SiLU()\n",
    "        #  ), k=8)\n",
    "        # self.edge_conv4 = DynamicEdgeConv(nn.Sequential(\n",
    "        #      nn.Linear(256, 256),\n",
    "        #      nn.SiLU(),\n",
    "        #      nn.Linear(256, 128),\n",
    "        #      nn.SiLU()\n",
    "        #  ), k=K)\n",
    "        self.fc = nn.Linear(128, 21)  # Predicting instance mask for each point\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.pos, data.edge_index\n",
    "        # print(data.edge_index)\n",
    "        # return\n",
    "        x = self.edge_conv1(x, edge_index)\n",
    "        # x = self.edge_conv2(x, edge_index)\n",
    "        # x = self.edge_conv3(x, edge_index)\n",
    "        # x = self.edge_conv4(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Calc number of trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(PointNetInstanceSeg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/home/group10/deephalo_gnn/Imbalance_Resampled_for_mulltilabel/train\"\n",
    "files = os.listdir(data)\n",
    "point_cloud_data = [(np.load(data+\"/\"+f)) for f in files if f.endswith(\".npy\")] # List of point cloud data, each element is a list of point coordinates\n",
    "\n",
    "# Convert each point cloud data into a Data object\n",
    "data_list = []\n",
    "for point_cloud in point_cloud_data:\n",
    "    # Create a Data object with the features\n",
    "    pos = torch.tensor(point_cloud[:,:-1], dtype=torch.float)\n",
    "    # Recentering positions per halo\n",
    "    pos[:3] = pos[:3] - pos[:3].mean(dim=1, keepdim=True)\n",
    "    data = Data(\n",
    "        pos=pos,\n",
    "        y = torch.eye(21)[torch.tensor(point_cloud[:,-1]+1, dtype=torch.long)],\n",
    "        \n",
    "    )\n",
    "    \n",
    "    data_list.append(data)\n",
    "\n",
    "# Now you can use DataLoader with this list of Data objects\n",
    "loader = DataLoader(data_list, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([0, 0, 0,  ..., 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "data.y\n",
    "print(data.y)\n",
    "class_labels = torch.argmax(data.y, dim=1)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for point_clouds in point_cloud_data:\n",
    "    label = point_clouds[:,-1]\n",
    "    labels.append(label)\n",
    "labels = torch.tensor(np.concatenate(labels))\n",
    "\n",
    "# Calculate unique labels and counts\n",
    "unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "\n",
    "# Calculate frequencies\n",
    "frequencies = counts.float() / labels.numel()\n",
    "\n",
    "# Calculate weights\n",
    "weight_vec = 1.0 / torch.log(torch.tensor(1.2) + frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_vec[0]=0.01\n",
    "weight_vec\n",
    "weight_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1., gamma=2.):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce_logits = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = self.bce_logits(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 102/102 [00:36<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 347.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 102/102 [00:39<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 117.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 102/102 [00:41<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 94.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 102/102 [00:38<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 79.5665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 102/102 [00:41<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 69.5066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 102/102 [00:28<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 53.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 102/102 [00:42<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 51.4408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 102/102 [00:56<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 42.6877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 102/102 [00:59<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 38.9577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 102/102 [00:56<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 27.4406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = PointNetInstanceSeg().to(DEVICE)\n",
    "weights = torch.FloatTensor(weight_vec).to(DEVICE)\n",
    "\n",
    "\n",
    "criterion = FocalLoss(alpha=weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        data.to(DEVICE)\n",
    "        # print(data.y.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * data.num_graphs\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Create the \"ckpts\" directory if it doesn't exist\n",
    "import os, time\n",
    "\n",
    "os.makedirs(\"ckpts\", exist_ok=True)\n",
    "curr_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{curr_time}_pointnet_instance_seg\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f\"./ckpts/{model_name}_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = \"/home/group10/deephalo_gnn/Labeled subhalo matrices of haloes/test\"\n",
    "files = os.listdir(data_test)\n",
    "point_cloud_data = [(np.load(data_test+\"/\"+f)) for f in files if f.endswith(\".npy\") and (int(f[:-4])>50)] # List of point cloud data, each element is a list of point coordinates\n",
    "\n",
    "# Convert each point cloud data into a Data object\n",
    "data_test_list = []\n",
    "for point_cloud in point_cloud_data:\n",
    "    # Create a Data object with the features\n",
    "    data_test = Data(pos=torch.tensor(point_cloud[:,:-1], dtype=torch.float), y = torch.eye(21)[torch.tensor(point_cloud[:,-1]+1, dtype=torch.long)])\n",
    "    data_test_list.append(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 25/25 [00:02<00:00, 10.30it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loader = DataLoader(data_test_list, batch_size=1, shuffle=False)\n",
    "    \n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions\n",
    "ground_truth_labels = []\n",
    "predictions = []\n",
    "pos_list = []\n",
    "\n",
    "# Loop over the test data\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader, desc='Testing'):\n",
    "        # Move data to the device\n",
    "        data = data.to(DEVICE)\n",
    "        \n",
    "        # Pass the data through the model\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Get the predicted labels\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        _, ground_truth = torch.max(data.y, 1)\n",
    "        pos = data.pos.cpu().numpy()\n",
    "        pos = pos[:,0:3]\n",
    "        # Store the predictions\n",
    "        ground_truth_labels.append(ground_truth.cpu().numpy())\n",
    "        predictions.append(predicted_labels.cpu().numpy())\n",
    "        pos_list.append(pos)\n",
    "    \n",
    "\n",
    "# At this point, `predictions` is a list of numpy arrays with the predicted labels for each point cloud in the test set\n",
    "# You can now compare these predictions to the actual labels to compute your test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_iou(pred, target):\n",
    "    pred = pred.float()\n",
    "    target = target.float()\n",
    "\n",
    "    # Reshape the tensors to a 2D format\n",
    "    pred = pred.view(pred.shape[0], -1)\n",
    "    target = target.view(target.shape[0], -1)\n",
    "\n",
    "    # Calculate intersection and union for each sample\n",
    "    intersection = (pred * target).sum(dim=1)\n",
    "    union = (pred + target).clamp(0, 1).sum(dim=1)\n",
    "\n",
    "    # Calculate IoU and avoid division by zero\n",
    "    iou = intersection / (union + 1e-8)\n",
    "\n",
    "    return iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid = 22\n",
    "\n",
    "# tar, masked = self.prep_tar(hid)\n",
    "# num_masked = sum(masked)\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter3d(\n",
    "        x=pos_list[hid][:,0],\n",
    "        y=pos_list[hid][:,1],\n",
    "        z=pos_list[hid][:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1, # Larger than surrounding data-points\n",
    "            color=ground_truth_labels[hid],\n",
    "            opacity=0.75,\n",
    "            showscale=True,\n",
    "        ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title=f'{predictions[hid].shape} particles', title_x=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar, masked = self.prep_tar(hid)\n",
    "# num_masked = sum(masked)\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter3d(\n",
    "        x=pos_list[hid][:,0],\n",
    "        y=pos_list[hid][:,1],\n",
    "        z=pos_list[hid][:,2],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1, # Larger than surrounding data-points\n",
    "            color=predictions[hid],\n",
    "            opacity=0.75,\n",
    "            showscale=True,\n",
    "        ))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title=f'{predictions[hid].shape} particles', title_x=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t (10065,) \t [ 0  1  3  4  5  7 10] \t [0 1 2 3]\n",
      "\n",
      "1 \t (5704,) \t [0 1 7] \t [0 1 2]\n",
      "\n",
      "2 \t (14113,) \t [1 7] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "\n",
      "3 \t (7083,) \t [0 1 3 4 5 6] \t [0 1 2 3 4 5]\n",
      "\n",
      "4 \t (10423,) \t [1 3 4 7 9] \t [0 1 2]\n",
      "\n",
      "5 \t (6830,) \t [1 7] \t [0 1 2 3 4 5 6]\n",
      "\n",
      "6 \t (6355,) \t [7] \t [0]\n",
      "\n",
      "7 \t (13648,) \t [0 1 7] \t [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "\n",
      "8 \t (12394,) \t [7] \t [0 1 2 3 4 5 6]\n",
      "\n",
      "9 \t (7559,) \t [0 1 7] \t [0 1 2 3 4 5]\n",
      "\n",
      "10 \t (16444,) \t [0 7] \t [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "11 \t (6586,) \t [ 0  1  3  4  7  8 10 14] \t [0 1 2 3]\n",
      "\n",
      "12 \t (9898,) \t [0 1 4 7] \t [0 1 2 3]\n",
      "\n",
      "13 \t (8716,) \t [0 1 4] \t [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "14 \t (6429,) \t [0 1 3 4 7] \t [0 1 2 3 4 5 6 7 8]\n",
      "\n",
      "15 \t (15265,) \t [ 0  1  3  4  5  7 10] \t [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "16 \t (18205,) \t [0 1 5 7] \t [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "\n",
      "17 \t (13278,) \t [0 1 4 5 7] \t [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "18 \t (15318,) \t [1 4 7] \t [0 1 2 3 4 5 6 7]\n",
      "\n",
      "19 \t (6788,) \t [ 0  1  3  4  5  9 11] \t [0 1 2 3 4 5]\n",
      "\n",
      "20 \t (9654,) \t [1 7] \t [0 1 2]\n",
      "\n",
      "21 \t (5874,) \t [ 0  1  3  4  7  8  9 11] \t [0 1]\n",
      "\n",
      "22 \t (5374,) \t [0 1 3 4 5 7] \t [0 1 2 3 4 5]\n",
      "\n",
      "23 \t (10569,) \t [0 1 3 4 7] \t [0 1 2 3 4 5 6]\n",
      "\n",
      "24 \t (5412,) \t [ 4  9 10] \t [0 1 2 3 4]\n",
      "\n",
      "Mean acc: 1.1313 \\pm 1.3251\n"
     ]
    }
   ],
   "source": [
    "iou_score = []\n",
    "for idx, (gt, pred) in enumerate(zip(ground_truth_labels, predictions)):\n",
    "    # Checking if the model predicts different labels for different points in the same point cloud\n",
    "    # if np.unique(pred).shape[0] != 1:\n",
    "    print(idx, \"\\t\", gt.shape, \"\\t\", np.unique(pred), \"\\t\", np.unique(gt))\n",
    "    print()\n",
    "    \n",
    "    iou_score.append(multi_label_iou(torch.tensor(pred), torch.tensor(gt)))\n",
    "\n",
    "print(f\"Mean acc: {np.mean(iou_score):.4f} \\pm {np.std(iou_score):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
